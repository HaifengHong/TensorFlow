# -*-coding:utf-8-*-
import tensorflow as tf
import numpy as np

# 使用numpy生成100个随机点
x_data = np.random.rand(100) # 一维向量，均匀分布在[0,1)之间
y_data = x_data * 0.1 + 0.2

# 构造一个线性模型
k = tf.Variable(0.) # 初始值，浮点型
b = tf.Variable(0.)
y = k * x_data + b

# 定义代价函数
loss = 0.5 * tf.reduce_mean(tf.square(y_data - y)) # 误差的平方再平均，注意加tf.

# 定义使用不同方法进行训练的优化器
# optimizer = tf.train.GradientDescentOptimizer(0.2) # 学习率0.2
# optimizer = tf.train.AdadeltaOptimizer(0.2)
optimizer = tf.train.MomentumOptimizer(0.2, 0.9) # 冲量通常设为0.9

# 定义一个训练，最小化代价函数
train = optimizer.minimize(loss)

# 初始化变量
init = tf.global_variables_initializer() # 注意加tf.

with tf.Session() as sess:
    sess.run(init)
    for step in range(201): # 迭代200次，注意是range(201)
        sess.run(train)
        if step % 20 == 0:
            print(step, sess.run([k,b])) # 每迭代20次看一下结果 # 注意表达方式sess.run([k,b])

# 最终结果应该接近于k=0.1,b=0.2

#  结果输出：
# tf.train.GradientDescentOptimizer(0.2)
# 0 [0.026193185, 0.049840726]
# 20 [0.102954715, 0.19787243]
# 40 [0.1024419, 0.19871147]
# 60 [0.10187031, 0.1990144]
# 80 [0.10143209, 0.19924533]
# 100 [0.10109654, 0.19942215]
# 120 [0.100839615, 0.19955754]
# 140 [0.100642905, 0.19966121]
# 160 [0.10049227, 0.19974057]
# 180 [0.10037691, 0.19980137]
# 200 [0.100288585, 0.19984794]

# tf.train.AdadeltaOptimizer(0.2)（未得到想要的结果）
# 0 [8.944223e-05, 8.9442576e-05]
# 20 [0.001962275, 0.0019626708]
# 40 [0.0038843267, 0.0038860145]
# 60 [0.0058304346, 0.0058343727]
# 80 [0.0077949087, 0.0078020827]
# 100 [0.009775264, 0.009786679]
# 120 [0.011769988, 0.011786666]
# 140 [0.013777913, 0.013800891]
# 160 [0.01579801, 0.015828343]
# 180 [0.01782932, 0.017868074]
# 200 [0.019870903, 0.01991917]

# tf.train.MomentumOptimizer(0.2, 0.9)
# 0 [0.031545628, 0.051483173]
# 20 [0.0791005, 0.17178632]
# 40 [0.11228425, 0.22202644]
# 60 [0.10090214, 0.19981949]
# 80 [0.09815828, 0.1974788]
# 100 [0.10023503, 0.20038]
# 120 [0.10018731, 0.20024715]
# 140 [0.09993519, 0.19991957]
# 160 [0.09999041, 0.19998062]
# 180 [0.100008875, 0.20001312]
# 200 [0.09999964, 0.2000004]